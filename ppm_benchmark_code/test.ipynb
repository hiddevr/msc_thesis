{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BenchmarkLoader\n",
    "\n",
    "The cell below shows the code for loading a benchmark from a configuration file. Click [here](benchmark_configs/test_config.yaml) to see the config file.\n",
    "\n",
    "Loading a benchmark performs all necessary processing steps including loading the data and generating benchmark tasks. To allow for downloading public \n",
    "data which requires attribution, downloading data from remote sources is supported. when a remote source is used, the proper credits are displayed to the\n",
    "user."
   ],
   "id": "85458782a6a2938d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:04:32.445712Z",
     "start_time": "2024-06-19T11:03:41.849355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ppm_benchmark.core.benchmark_loader import BenchmarkLoader\n",
    "\n",
    "\n",
    "loader = BenchmarkLoader()\n",
    "\n",
    "benchmark = loader.load_from_config('ppm_benchmark/benchmark_configs/test_config.yaml')"
   ],
   "id": "a5c95af7a80ca1c4",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading Benchmark & Tasks\n",
    "\n",
    "When a benchmark is generated from a config file it is automatically saved to disk. The `.load_from_folder()` method can be used to retrieve the configured benchmark.\n",
    "Each benchmark revolves around several tasks on which performance can be measured. The `.get_tasks()` method returns all task names, these can be used to retrieve the training and testing data."
   ],
   "id": "c1cb6ef6c75791a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:06:14.488649Z",
     "start_time": "2024-06-19T11:06:14.479347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from ppm_benchmark.core.benchmark_loader import BenchmarkLoader\n",
    "\n",
    "\n",
    "loader = BenchmarkLoader()\n",
    "benchmark = loader.load_from_folder('test_benchmark')\n",
    "\n",
    "tasks = benchmark.get_tasks()\n",
    "tasks "
   ],
   "id": "19ce33ab58425783",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting Task Data\n",
    "\n",
    "The cell below shows the code for retrieving the training and test data. This simple example is a classification task for the last activity. For this example the 'prediction' is a randomly selected activity."
   ],
   "id": "759fcfc989e46f28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:06:21.707445Z",
     "start_time": "2024-06-19T11:06:16.521636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "task = benchmark.load_task('test_task')\n",
    "\n",
    "preds = dict()\n",
    "train_df = task.get_train_data()\n",
    "test_df = task.get_test_data()\n",
    "targets = train_df['concept:name'].unique().tolist()\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "        pred = random.choice(targets)\n",
    "        case_id = row['case:concept:name']\n",
    "        preds[case_id] = pred\n",
    "preds"
   ],
   "id": "37c2bd456605f8b0",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is done for each individual task. The evaluation functions are specified in the config and will be displayed to the user when running the code below."
   ],
   "id": "4342c7cc41779bf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:06:21.759417Z",
     "start_time": "2024-06-19T11:06:21.709488Z"
    }
   },
   "cell_type": "code",
   "source": "benchmark.evaluate(task, preds)",
   "id": "4cdcab8d9eb7439",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "\n",
    "The experiment class is implemented to help users in tracking PPM experiments. A run can be initialized to track a task. If the optional parameter `model_type=tensorflow` is given to the `init_run()` method, the returned run_tracker is a Tensorflow callback which supports automatic tracking. For other model types, the generic run_tracker can be used inside the training loop for tracking metrics. The code below simulates a situation for tracking the loss."
   ],
   "id": "2c914d6ce278921a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T13:35:37.101887Z",
     "start_time": "2024-06-06T13:35:19.861038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ppm_benchmark.core.experiment import Experiment\n",
    "\n",
    "\n",
    "experiment = Experiment()\n",
    "experiment.new_experiment('test_experiment')\n",
    "\n",
    "task = benchmark.load_task('test_task')\n",
    "run_tracker, run = experiment.init_run(task, 'test_run')\n",
    "\n",
    "for i in range(0, 10):\n",
    "        run_tracker.epoch_end({'loss': 0})\n",
    "\n",
    "run_tracker.train_end()\n",
    "run_tracker.evaluate(preds)\n"
   ],
   "id": "39779b39e5719a9f",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieving Run Data\n",
    "\n",
    "using the `to_dict()` method on a run object returns all tracked run data in a dictionary format. This can easily be converted into a DataFrame for further analysis."
   ],
   "id": "36a6e03ff972b27a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T13:35:37.117475Z",
     "start_time": "2024-06-06T13:35:37.101887Z"
    }
   },
   "cell_type": "code",
   "source": "run.to_dict()",
   "id": "d62786c9d4a9dea3",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "f1c669c3a305937b",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
